# Agent-Multitool-Demo
Agente conversacional en Python que usa Ollama para ejecutar modelos de lenguaje en local. Permite mantener diÃ¡logos interactivos, llamar a herramientas personalizadas desde tools.py y seleccionar automÃ¡ticamente el mejor modelo disponible, ideal para asistentes offline sin depender de servicios en la nube.

ðŸ§  Ollama Local Agent

Este repositorio contiene un agente conversacional en Python que se ejecuta en local utilizando Ollama
 como backend de modelos de lenguaje.
El agente es capaz de:

Mantener una conversaciÃ³n interactiva.

Usar herramientas externas (definidas en tools.py) para buscar informaciÃ³n o ejecutar funciones.

Seleccionar automÃ¡ticamente un modelo vÃ¡lido entre varias opciones disponibles.

ðŸ“‹ Requisitos previos

Antes de ejecutar el proyecto, asegÃºrate de contar con:

Python 3.10 o superior instalado.

Ollama instalado y en ejecuciÃ³n en tu mÃ¡quina.

Descarga e instalaciÃ³n: https://ollama.ai/download

Al menos un modelo compatible descargado, por ejemplo:

ollama pull qwen3:8b


ðŸ’¡ Puedes consultar los modelos disponibles en tu sistema con:

ollama list

âš™ï¸ InstalaciÃ³n de dependencias

Clona el repositorio e instala las dependencias necesarias:

git clone https://github.com/tuusuario/ollama-local-agent.git
cd ollama-local-agent
pip install -r requirements.txt


El archivo requirements.txt debe incluir al menos:

ollama


(Agrega cualquier otra librerÃ­a que uses en tools.py si corresponde.)

ðŸš€ EjecuciÃ³n del agente

(Opcional) Define la variable de entorno para elegir un modelo especÃ­fico:

export MODEL="qwen3:8b"


Si no lo haces, el script intentarÃ¡ usar por defecto:

qwen3:8b â†’ deepseek-r1:8b â†’ qwen2.5 â†’ llama3.1:8b


Ejecuta el programa:

python main.py


Cuando el agente estÃ© listo, escribe tus consultas:

> Inserte su consulta aquÃ­: Hola, Â¿quÃ© puedes hacer?


Escribe quit o exit para salir.

ðŸ“‚ Estructura del proyecto
.
â”œâ”€ main.py          # CÃ³digo principal del agente
â”œâ”€ tools.py         # DefiniciÃ³n de herramientas (TOOL_SPECS y TOOL_FUNCS)
â”œâ”€ requirements.txt # Dependencias del proyecto
â””â”€ README.md        # Este archivo

tools.py

En este archivo defines:

TOOL_SPECS: DescripciÃ³n de las herramientas disponibles para el modelo (nombre, argumentos, descripciÃ³n).

TOOL_FUNCS: ImplementaciÃ³n de cada herramienta como funciones de Python.

Ejemplo de una herramienta simple:

TOOL_SPECS = [
    {
        "function": {
            "name": "sumar",
            "description": "Suma dos nÃºmeros.",
            "parameters": {
                "a": {"type": "number"},
                "b": {"type": "number"}
            }
        }
    }
]

def sumar(a, b):
    return a + b

TOOL_FUNCS = {"sumar": sumar}

ðŸ”§ Funcionamiento interno

SelecciÃ³n de modelo:
El script intenta conectarse al modelo indicado en MODEL.
Si falla, recorre una lista de candidatos (qwen3:8b, deepseek-r1:8b, qwen2.5, llama3.1:8b) hasta encontrar uno disponible.

InteracciÃ³n:
El usuario escribe una consulta.
El modelo puede:

Responder directamente.

Solicitar el uso de una herramienta (definida en tools.py).

Herramientas:
Cuando el modelo solicita una herramienta, el cÃ³digo ejecuta la funciÃ³n correspondiente en Python y devuelve el resultado al modelo para que continÃºe el razonamiento.
